{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "local-authorization",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "@author: Vu Hoang Viet\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from utils import lmdb_dataset_loader\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "mineral-acrobat",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(torch.tensor([1,2,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "liberal-darwin",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Extraction(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Extraction, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=4, kernel_size=4, stride=1, bias=False),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(in_channels=4, out_channels=4, kernel_size=4, stride=1, bias=False),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=4, stride=2),\n",
    "            nn.Conv2d(in_channels=4, out_channels=16, kernel_size=4, stride=1, bias=False),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(in_channels=16, out_channels=16, kernel_size=4, stride=1, bias=False),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=4, stride=1),\n",
    "            nn.Conv2d(in_channels=16, out_channels=64, kernel_size=4,stride=1, bias=False),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=4,stride=1, bias=False),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=4, stride=2),\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "\n",
    "        @param input:\n",
    "        @return:\n",
    "        \"\"\"\n",
    "        output = self.model(input)\n",
    "        return output\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_dim, max_features = 10):\n",
    "        super(Generator, self).__init__()\n",
    "        self.device = torch.device(\"cuda:0\" if (torch.cuda.is_available()) else \"cpu\")\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.max_features = max_features\n",
    "        self.extractor = Extraction()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=64*10, out_channels=64, kernel_size=4, stride=2, bias=False),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(in_channels=64, out_channels=64, kernel_size=4, stride=2, bias=False),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(in_channels=64, out_channels=16, kernel_size=4, stride=2, bias=False),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(in_channels=16, out_channels=16, kernel_size=3, stride=1, bias=False),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(in_channels=16, out_channels=4, kernel_size=3, stride=1, bias=False),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(in_channels=4, out_channels=4, kernel_size=4, stride=1, bias=False),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(in_channels=4, out_channels=1, kernel_size=4, stride=1, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, input_imgs):\n",
    "        \"\"\"\n",
    "        @param input:\n",
    "        @return:\n",
    "        \"\"\"\n",
    "        for input_img in input_imgs:\n",
    "            input_img = torch.tensor(input_img).to(self.device)\n",
    "            features_out = self.extractor(input_img.float())\n",
    "            empty = torch.zeros((self.max_features - len(features_out),\n",
    "                                 features_out.shape[1],\n",
    "                                 features_out.shape[2],\n",
    "                                 features_out.shape[3])).to(self.device)\n",
    "            features_out = torch.cat(\n",
    "                [features_out,empty\n",
    "                 ],dim  = 0)\n",
    "            features_out = features_out.reshape(1, 64*self.max_features,features_out.shape[2], features_out.shape[3])\n",
    "            try:\n",
    "                features_outs = torch.cat(\n",
    "                    [features_outs,features_out\n",
    "                     ],dim  = 0)\n",
    "            except:\n",
    "                features_outs = features_out\n",
    "        output = self.model(torch.tensor(features_outs))\n",
    "        return output\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.feature_extraction = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, bias=False),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3, stride=1, bias=False),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=4, stride=2),\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, bias=False),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, stride=1, bias=False),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=4, stride=2),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, bias=False),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, bias=False),\n",
    "            nn.ReLU(True),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        self.fake_real_model = nn.Linear(3136, 2)\n",
    "        self.property_model = nn.Linear(3136, num_classes)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        @param input:\n",
    "        @return:\n",
    "        \"\"\"\n",
    "        feature = self.feature_extraction(input)\n",
    "        fake_real = self.fake_real_model(feature)\n",
    "        property = self.property_model(feature)\n",
    "        return fake_real, property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "analyzed-office",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HandWrittenGan:\n",
    "    \"\"\"\n",
    "    Model class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_dim,\n",
    "                 target_size=(64, 64),\n",
    "                 input_path=None,\n",
    "                 target_path=None,\n",
    "                 classes=None,\n",
    "                 batch_size=1,\n",
    "                 max_features=10,\n",
    "                 max_iter=1e+5,\n",
    "                 check_times = 1e+3):\n",
    "        self.device = torch.device(\"cuda:0\" if (torch.cuda.is_available()) else \"cpu\")\n",
    "        self.input_dim = input_dim\n",
    "        self.classes = classes\n",
    "        self.num_classes = len(classes)\n",
    "        self.max_features = max_features\n",
    "        self.real_label = 1\n",
    "        self.fake_label = 0\n",
    "        \n",
    "        self.input_data = lmdb_dataset_loader(input_path)\n",
    "        self.target_data = lmdb_dataset_loader(target_path)\n",
    "        self.target_size = target_size\n",
    "        self.batch_size = batch_size\n",
    "        self.max_iter = max_iter\n",
    "        self.check_times = check_times\n",
    "        \n",
    "        self.generator = Generator(input_dim).to(self.device)\n",
    "        self.discriminator = Discriminator(self.num_classes).to(self.device)\n",
    "        self.logs = {}\n",
    "\n",
    "    def data_loader(self):\n",
    "        while True:\n",
    "            labels = []\n",
    "            input_imgs = []\n",
    "            real_imgs = []\n",
    "            for _ in range(self.batch_size):\n",
    "                label_ind = np.random.randint(self.num_classes)\n",
    "                label = self.classes[label_ind]\n",
    "                input_img = []\n",
    "                for _ in range(2):\n",
    "                    img = np.array(self.input_data.random_sample(label))\n",
    "                    img = cv2.resize(img, self.target_size, interpolation = cv2.INTER_AREA)\n",
    "                    img = np.reshape(img, (1,self.target_size[0],self.target_size[1]))\n",
    "                    input_img.append(img)\n",
    "                for i in range(2,10):\n",
    "                    if np.random.rand() < 0.5:\n",
    "                        img = np.array(self.input_data.random_sample(label))\n",
    "                        img = cv2.resize(img, self.target_size, interpolation = cv2.INTER_AREA)\n",
    "                        img = np.reshape(img, (1,self.target_size[0],self.target_size[1]))\n",
    "                    else:\n",
    "                        img = np.zeros_like(img)\n",
    "                    input_img.append(img)\n",
    "                real_img = np.array(self.target_data.random_sample(label))\n",
    "                real_img = cv2.resize(real_img, self.target_size, interpolation = cv2.INTER_AREA)\n",
    "                real_img = np.reshape(real_img, (1,self.target_size[0],self.target_size[1]))\n",
    "                labels.append(label_ind)\n",
    "                input_imgs.append(input_img)\n",
    "                real_imgs.append(real_img)\n",
    "            yield np.array(labels), np.array(input_imgs), np.array(real_imgs)\n",
    "\n",
    "    def trainer(self, epochs):\n",
    "        \"\"\"\n",
    "\n",
    "        @return:\n",
    "        \"\"\"\n",
    "        optimizerD = optim.Adam(self.discriminator.parameters(), lr=1e-3, betas=(0.1, 0.999))\n",
    "        optimizerG = optim.Adam(self.generator.parameters(), lr=1e-3, betas=(0.1, 0.999))\n",
    "#         optimizerD = optim.SGD(self.discriminator.parameters(), lr=1e-2, momentum=0.9)\n",
    "#         optimizerG = optim.SGD(self.generator.parameters(), lr=1e-2, momentum=0.9)\n",
    "        self.logs['d_loss_real'] = []\n",
    "        self.logs['d_loss_fake'] = []\n",
    "        self.logs['d_loss_cls_real'] = []\n",
    "        loss_d1 = nn.CrossEntropyLoss()\n",
    "        loss_d2 = nn.CrossEntropyLoss()\n",
    "        loss_g = nn.CrossEntropyLoss()\n",
    "        running_d_loss = 0\n",
    "        running_g_loss = 0\n",
    "        real_loss_plot = []\n",
    "        fake_loss_plot = []\n",
    "        mean_loss_plot = []\n",
    "        for epoch in range(epochs):\n",
    "            count = 0\n",
    "            b = time.time()\n",
    "            for labels, input_imgs, real_imgs in self.data_loader():\n",
    "                if count >= self.max_iter:\n",
    "                    break\n",
    "                else:\n",
    "                    count += 1\n",
    "                optimizerD.zero_grad()\n",
    "                real_imgs = torch.tensor(real_imgs).to(self.device)\n",
    "\n",
    "                # Đánh label true fake images\n",
    "                true_label = torch.full((self.batch_size,), self.real_label,dtype=torch.long,\n",
    "                                             device=self.device)\n",
    "                fake_label = torch.full((self.batch_size,), self.fake_label,dtype=torch.long,\n",
    "                                             device=self.device)\n",
    "\n",
    "                # Đánh label phân loại\n",
    "                property_label = torch.tensor(labels,dtype=torch.long,\n",
    "                                             device=self.device)\n",
    "\n",
    "                # Training model D với ảnh reals\n",
    "#                 for real_img in real_imgs:\n",
    "#                     plt.imshow(real_img.cpu().detach().numpy()[0])\n",
    "#                     plt.show()\n",
    "                real_imgs = real_imgs.float().to(self.device)\n",
    "                out_real_src, out_real_cls = self.discriminator(real_imgs)\n",
    "                d_loss_real = loss_d1(out_real_src, true_label)\n",
    "                d_loss_cls_real = loss_d2(out_real_cls, property_label)\n",
    "\n",
    "#                 # Tạo ảnh fake bằng G-models từ features từ E-Models\n",
    "                input_imgs = torch.tensor(input_imgs)\n",
    "                x_fake = self.generator(input_imgs)\n",
    "\n",
    "# #                 # Training models D với ảnh fake\n",
    "                out_fake_src, _ = self.discriminator(x_fake.detach())\n",
    "                d_loss_fake = loss_d1(out_fake_src,fake_label)\n",
    "                \n",
    "                d_loss = d_loss_real + d_loss_cls_real\n",
    "                d_loss.backward()\n",
    "                optimizerD.step()\n",
    "                running_d_loss += d_loss.item()\n",
    "                \n",
    "#                 if count % (self.check_times//10) == 0:\n",
    "                optimizerG.zero_grad()\n",
    "                x_fake = self.generator(input_imgs)\n",
    "                out_fake_src, out_fake_cls = self.discriminator(x_fake.detach())\n",
    "                g_loss_fake = loss_d1(out_fake_src, true_label)\n",
    "                g_loss_cls_fake = loss_g(out_fake_cls, property_label)\n",
    "                g_loss = g_loss_fake + g_loss_cls_fake\n",
    "                running_g_loss += g_loss.item()\n",
    "                g_loss.backward()\n",
    "                optimizerG.step() \n",
    "                    \n",
    "                if count % self.check_times == 0:\n",
    "                    d_loss = running_d_loss/self.check_times\n",
    "                    g_loss = running_g_loss/(self.check_times//10)\n",
    "                    real_loss_plot.append(d_loss)\n",
    "                    fake_loss_plot.append(g_loss)\n",
    "                    mean_loss_plot.append((d_loss+g_loss)/2)\n",
    "                    print(\"[%3d,%10d], D_Loss: %10f, G_Loss: %10f, Mean Loss: %5f, Seconds/batch: %5f\"%(epoch, count,d_loss, g_loss, (d_loss+g_loss)/2, time.time() - b))\n",
    "                    b = time.time()\n",
    "                    running_d_loss = 0\n",
    "                    running_g_loss = 0  \n",
    "                    if count % (self.check_times*10) == 0:\n",
    "                        plt.subplot(2, 1, 1)\n",
    "                        plt.plot(real_loss_plot, label='d_loss')\n",
    "                        plt.plot(fake_loss_plot, label='g_loss')\n",
    "                        plt.plot(mean_loss_plot, label='mean_loss')\n",
    "                        plt.show()\n",
    "            fig=plt.figure(figsize=(4, 2))\n",
    "            fig.add_subplot(1, 2, 1)\n",
    "            plt.imshow(real_imgs[0][0])\n",
    "            fig.add_subplot(1, 2, 2)\n",
    "            plt.imshow(x_fake.cpu().detach().numpy()[0][0])\n",
    "            plt.show()\n",
    "            print(\"Epochs %2d finished.\"%epoch)\n",
    "                    \n",
    "\n",
    "    def predictor(self):\n",
    "        \"\"\"\n",
    "\n",
    "        @return:\n",
    "        \"\"\"\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "brief-estate",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = HandWrittenGan(\n",
    "                 input_dim = 5,\n",
    "                 target_size=(64, 64),\n",
    "                 input_path='/home/vietvh9/Project/OCR_Components/data/sources/synthtext_font_lmdb',\n",
    "                 target_path='/home/vietvh9/Project/OCR_Components/data/sources/ETL_NIST_processed_v2.0_lmdb',\n",
    "                 classes=[str(i) for i in range(2)],\n",
    "                 batch_size=10,\n",
    "                 max_features=10,\n",
    "                 max_iter=1e+5,\n",
    "                 check_times = 1e+2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "taken-depression",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vietvh9/Project/.envs/GAN/lib/python3.6/site-packages/ipykernel_launcher.py:65: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/vietvh9/Project/.envs/GAN/lib/python3.6/site-packages/ipykernel_launcher.py:81: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0,       100], D_Loss:   0.284970, G_Loss:  13.880105, Mean Loss: 7.082537, Seconds/batch: 7.508154\n",
      "[  0,       200], D_Loss:   0.166305, G_Loss:  13.880380, Mean Loss: 7.023342, Seconds/batch: 7.440787\n",
      "[  0,       300], D_Loss:   1.410330, G_Loss:  13.870677, Mean Loss: 7.640503, Seconds/batch: 7.466636\n",
      "[  0,       400], D_Loss:   0.032037, G_Loss:  13.865704, Mean Loss: 6.948870, Seconds/batch: 7.411375\n",
      "[  0,       500], D_Loss:   0.025133, G_Loss:  13.867109, Mean Loss: 6.946121, Seconds/batch: 7.413241\n",
      "[  0,       600], D_Loss:   0.020757, G_Loss:  13.865495, Mean Loss: 6.943126, Seconds/batch: 7.383502\n",
      "[  0,       700], D_Loss:   0.054540, G_Loss:  13.865478, Mean Loss: 6.960009, Seconds/batch: 7.404816\n",
      "[  0,       800], D_Loss:   0.018969, G_Loss:  13.857545, Mean Loss: 6.938257, Seconds/batch: 7.448811\n",
      "[  0,       900], D_Loss:   0.019169, G_Loss:  13.857027, Mean Loss: 6.938098, Seconds/batch: 7.463930\n",
      "[  0,      1000], D_Loss:   0.047619, G_Loss:  13.857093, Mean Loss: 6.952356, Seconds/batch: 7.430336\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0,      1100], D_Loss:   0.015587, G_Loss:  13.851405, Mean Loss: 6.933496, Seconds/batch: 7.495515\n"
     ]
    }
   ],
   "source": [
    "models.trainer(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "given-breeding",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "input = torch.tensor([[0.9,0.1,0],[0,1,0]],dtype = torch.long)\n",
    "target = torch.tensor([0,1],dtype = torch.long)\n",
    "output = loss(input, target)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "front-egyptian",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = loss(input, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opposed-sleeve",
   "metadata": {},
   "outputs": [],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collect-stable",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
