{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "local-authorization",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "@author: Vu Hoang Viet\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from utils import lmdb_dataset_loader\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "liberal-darwin",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Extraction(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Extraction, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=4, kernel_size=4, stride=1, bias=False),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(in_channels=4, out_channels=4, kernel_size=4, stride=1, bias=False),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=4, stride=2),\n",
    "            nn.Conv2d(in_channels=4, out_channels=16, kernel_size=4, stride=1, bias=False),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(in_channels=16, out_channels=16, kernel_size=4, stride=1, bias=False),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=4, stride=1),\n",
    "            nn.Conv2d(in_channels=16, out_channels=64, kernel_size=4,stride=1, bias=False),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=4,stride=1, bias=False),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=4, stride=2),\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "\n",
    "        @param input:\n",
    "        @return:\n",
    "        \"\"\"\n",
    "        output = self.model(input)\n",
    "        return output\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, max_features = 10):\n",
    "        super(Generator, self).__init__()\n",
    "        self.device = torch.device(\"cuda:0\" if (torch.cuda.is_available()) else \"cpu\")\n",
    "        self.max_features = max_features\n",
    "        \n",
    "        self.extractor = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=4, kernel_size=4, stride=1, bias=False),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(in_channels=4, out_channels=4, kernel_size=4, stride=1, bias=False),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=4, stride=2),\n",
    "            nn.Conv2d(in_channels=4, out_channels=16, kernel_size=4, stride=1, bias=False),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(in_channels=16, out_channels=16, kernel_size=4, stride=1, bias=False),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=4, stride=1),\n",
    "            nn.Conv2d(in_channels=16, out_channels=64, kernel_size=4,stride=1, bias=False),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=4,stride=1, bias=False),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=4, stride=2),\n",
    "        )\n",
    "        \n",
    "        self.generate = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=64*self.max_features, out_channels=64, kernel_size=4, stride=2, bias=False),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(in_channels=64, out_channels=64, kernel_size=4, stride=2, bias=False),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(in_channels=64, out_channels=16, kernel_size=4, stride=2, bias=False),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(in_channels=16, out_channels=16, kernel_size=3, stride=1, bias=False),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(in_channels=16, out_channels=4, kernel_size=3, stride=1, bias=False),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(in_channels=4, out_channels=4, kernel_size=4, stride=1, bias=False),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(in_channels=4, out_channels=1, kernel_size=4, stride=1, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, input_imgs):\n",
    "        \"\"\"\n",
    "        @param input:\n",
    "        @return:\n",
    "        \"\"\"\n",
    "        for input_img in input_imgs:\n",
    "            input_img = torch.tensor(input_img).to(self.device)\n",
    "            features_out = self.extractor(input_img.float())\n",
    "            empty = torch.zeros((self.max_features - len(features_out),\n",
    "                                 features_out.shape[1],\n",
    "                                 features_out.shape[2],\n",
    "                                 features_out.shape[3])).to(self.device)\n",
    "            features_out = torch.cat(\n",
    "                [features_out,empty\n",
    "                 ],dim  = 0)\n",
    "            features_out = features_out.reshape(1, 64*self.max_features,features_out.shape[2], features_out.shape[3])\n",
    "            try:\n",
    "                features_outs = torch.cat(\n",
    "                    [features_outs,features_out\n",
    "                     ],dim  = 0)\n",
    "            except:\n",
    "                features_outs = features_out\n",
    "        output = self.generate(torch.tensor(features_outs))\n",
    "        return output\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.feature_extraction = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, bias=False),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3, stride=1, bias=False),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=4, stride=2),\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, bias=False),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, stride=1, bias=False),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=4, stride=2),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, bias=False),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, bias=False),\n",
    "            nn.ReLU(True),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        self.fake_real_model = nn.Linear(3136, 1)\n",
    "        self.property_model = nn.Linear(3136, num_classes)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        @param input:\n",
    "        @return:\n",
    "        \"\"\"\n",
    "        feature = self.feature_extraction(input)\n",
    "        fake_real = F.sigmoid(self.fake_real_model(feature))\n",
    "#         fake_real = self.fake_real_model(feature)\n",
    "        property = self.property_model(feature)\n",
    "        return fake_real, property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "analyzed-office",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HandWrittenGan:\n",
    "    \"\"\"\n",
    "    Model class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 target_size=(64, 64),\n",
    "                 input_path=None,\n",
    "                 target_path=None,\n",
    "                 classes=None,\n",
    "                 batch_size=1,\n",
    "                 max_features=10,\n",
    "                 max_iter=1e+5,\n",
    "                 check_times = 1e+3):\n",
    "        self.device = torch.device(\"cuda:0\" if (torch.cuda.is_available()) else \"cpu\")\n",
    "        self.classes = classes\n",
    "        self.num_classes = len(classes)\n",
    "        self.max_features = max_features\n",
    "        self.real_label = 1\n",
    "        self.fake_label = 0\n",
    "        \n",
    "        self.input_data = lmdb_dataset_loader(input_path)\n",
    "        self.target_data = lmdb_dataset_loader(target_path)\n",
    "        self.target_size = target_size\n",
    "        self.batch_size = batch_size\n",
    "        self.max_iter = max_iter\n",
    "        self.check_times = check_times\n",
    "        \n",
    "        self.generator = Generator(max_features).to(self.device)\n",
    "        self.discriminator = Discriminator(self.num_classes).to(self.device)\n",
    "        self.logs = {}\n",
    "\n",
    "    def data_loader(self):\n",
    "        while True:\n",
    "            labels = []\n",
    "            input_imgs = []\n",
    "            real_imgs = []\n",
    "            for _ in range(self.batch_size):\n",
    "                label_ind = np.random.randint(self.num_classes)\n",
    "                label = self.classes[label_ind]\n",
    "                input_img = []\n",
    "                for _ in range(2):\n",
    "                    img = np.array(self.input_data.random_sample(label))\n",
    "                    img = cv2.resize(img, self.target_size, interpolation = cv2.INTER_AREA)\n",
    "                    img = np.reshape(img, (1,self.target_size[0],self.target_size[1]))\n",
    "                    input_img.append(img)\n",
    "                for i in range(3,self.max_features):\n",
    "                    if np.random.rand() < 0.5:\n",
    "                        img = np.array(self.input_data.random_sample(label))\n",
    "                        img = cv2.resize(img, self.target_size, interpolation = cv2.INTER_AREA)\n",
    "                        img = np.reshape(img, (1,self.target_size[0],self.target_size[1]))\n",
    "                    else:\n",
    "                        img = np.zeros_like(img)\n",
    "                    input_img.append(img)\n",
    "                real_img = np.array(self.target_data.random_sample(label))\n",
    "                real_img = cv2.resize(real_img, self.target_size, interpolation = cv2.INTER_AREA)\n",
    "                real_img = np.reshape(real_img, (1,self.target_size[0],self.target_size[1]))\n",
    "                labels.append(label_ind)\n",
    "                input_imgs.append(input_img)\n",
    "                real_imgs.append(real_img)\n",
    "            yield np.array(labels), np.array(input_imgs)/255., np.array(real_imgs)/255.\n",
    "\n",
    "    def trainer(self, epochs):\n",
    "        \"\"\"\n",
    "\n",
    "        @return:\n",
    "        \"\"\"\n",
    "        optimizerD = optim.Adam(self.discriminator.parameters(), lr=1e-4, betas=(0.5, 0.9))\n",
    "        optimizerG = optim.Adam(self.generator.parameters(), lr=1e-4, betas=(0.5, 0.9))\n",
    "#         optimizerD = optim.SGD(self.discriminator.parameters(), lr=1e-3, momentum=0.9)\n",
    "#         optimizerG = optim.SGD(self.generator.parameters(), lr=1e-3, momentum=0.9)\n",
    "        self.logs['d_loss_real'] = []\n",
    "        self.logs['d_loss_fake'] = []\n",
    "        self.logs['d_loss_cls_real'] = []\n",
    "        loss_d1 = nn.MSELoss()\n",
    "        loss_d2 = nn.CrossEntropyLoss()\n",
    "        loss_g1 = nn.MSELoss()\n",
    "        loss_g2 = nn.CrossEntropyLoss()\n",
    "        l1_loss = nn.L1Loss()\n",
    "        running_d_loss = 0\n",
    "        running_g_loss = 0\n",
    "        real_loss_plot = []\n",
    "        fake_loss_plot = []\n",
    "        mean_loss_plot = []\n",
    "        for epoch in range(epochs):\n",
    "            count = 0\n",
    "            b = time.time()\n",
    "            for labels, input_imgs, real_imgs in self.data_loader():\n",
    "                if count >= self.max_iter:\n",
    "                    break\n",
    "                else:\n",
    "                    count += 1\n",
    "                optimizerD.zero_grad()\n",
    "                real_imgs = torch.tensor(real_imgs).to(self.device)\n",
    "\n",
    "                # Đánh label true fake images\n",
    "                true_label = torch.full((self.batch_size,), self.real_label,dtype=torch.float,\n",
    "                                             device=self.device)\n",
    "                fake_label = torch.full((self.batch_size,), self.fake_label,dtype=torch.float,\n",
    "                                             device=self.device)\n",
    "\n",
    "                # Đánh label phân loại\n",
    "                property_label = torch.tensor(labels,dtype=torch.long,\n",
    "                                             device=self.device)\n",
    "\n",
    "                # Training model D với ảnh reals\n",
    "#                 for real_img in real_imgs:\n",
    "#                     plt.imshow(real_img.cpu().detach().numpy()[0])\n",
    "#                     plt.show()\n",
    "                real_imgs = real_imgs.float().to(self.device)\n",
    "                out_real_src, out_real_cls = self.discriminator(real_imgs)\n",
    "\n",
    "                # Tạo ảnh fake bằng G-models từ features từ E-Models\n",
    "                input_imgs = torch.tensor(input_imgs)\n",
    "                x_fake = self.generator(input_imgs)\n",
    "        \n",
    "                out_fake_src, _ = self.discriminator(x_fake.detach())\n",
    "                src = torch.cat((out_real_src, out_fake_src), axis = 0)     \n",
    "                src = src.view(-1)\n",
    "                label = torch.cat((true_label, fake_label), axis = 0)            \n",
    "                \n",
    "                d_loss_real = loss_d1(src, label)\n",
    "#                 d_loss_real = torch.mean(1-out_real_src)+torch.mean(out_fake_src)\n",
    "                d_loss_cls_real = loss_d2(out_real_cls, property_label)\n",
    "\n",
    "# #                 # Training models D với ảnh fake\n",
    "                \n",
    "                d_loss = d_loss_real + d_loss_cls_real\n",
    "                d_loss.backward()\n",
    "#                 d_loss_fake.backward()\n",
    "                optimizerD.step()\n",
    "                \n",
    "                running_d_loss += d_loss.item()\n",
    "#                 running_d_loss += d_loss_fake.item()\n",
    "                \n",
    "#                 if count % (self.check_times//10) == 0:\n",
    "                g_labels, g_input_imgs, g_real_imgs = next(self.data_loader())\n",
    "                optimizerG.zero_grad()\n",
    "                input_imgs = torch.tensor(g_input_imgs)\n",
    "                x_fake = self.generator(g_input_imgs)\n",
    "                out_fake_src, out_fake_cls = self.discriminator(x_fake.detach())\n",
    "                out_fake_src = out_fake_src.view(-1)\n",
    "                g_loss_fake = loss_g1(out_fake_src, true_label)\n",
    "#                     g_loss_fake = torch.mean(1-out_fake_src)\n",
    "                g_real_imgs = torch.tensor(g_real_imgs).to(self.device)\n",
    "                g_real_imgs = g_real_imgs.float().to(self.device)\n",
    "                g_property_label = torch.tensor(g_labels,dtype=torch.long,\n",
    "                                             device=self.device)\n",
    "                g_loss_cls_fake = loss_g2(out_fake_cls, g_property_label)\n",
    "                loss_G_L1 = l1_loss(x_fake, g_real_imgs)\n",
    "                g_loss = g_loss_fake + loss_G_L1*10 + g_loss_cls_fake\n",
    "                g_loss.backward()\n",
    "                optimizerG.step() \n",
    "                running_g_loss += g_loss.item()\n",
    "                    \n",
    "                if count % self.check_times == 0:\n",
    "#                     print(out_real_cls)\n",
    "#                     print(property_label)\n",
    "#                     print(d_loss_cls_real)\n",
    "                    d_loss = running_d_loss/self.check_times\n",
    "                    g_loss = running_g_loss/self.check_times\n",
    "                    real_loss_plot.append(d_loss)\n",
    "                    fake_loss_plot.append(g_loss)\n",
    "                    mean_loss_plot.append((d_loss+g_loss)/2)\n",
    "                    print(\"[%3d,%10d], D_Loss: %10f, G_Loss: %10f, Mean Loss: %5f, Seconds/batch: %5f\"%(epoch, count,d_loss, g_loss, (d_loss+g_loss)/2, (time.time() - b)/self.check_times))\n",
    "                    b = time.time()\n",
    "                    running_d_loss = 0\n",
    "                    running_g_loss = 0  \n",
    "                    if count % (self.check_times*10) == 0:\n",
    "                        print(\"\\n\"*5)\n",
    "#                         print(src)\n",
    "#                         print(out_fake_src)\n",
    "#                         plt.subplot(2, 1, 1)\n",
    "                        plt.plot(real_loss_plot, label='d_loss')\n",
    "                        plt.plot(fake_loss_plot, label='g_loss')\n",
    "                        plt.plot(mean_loss_plot, label='mean_loss')\n",
    "                        plt.show()\n",
    "                        if count % (self.check_times*100) == 0:\n",
    "                            for j in range(len(real_imgs)):\n",
    "                                fig=plt.figure(figsize=(4, 2))\n",
    "                                fig.add_subplot(1, 2, 1)\n",
    "                                plt.imshow(real_imgs[j][0].cpu().detach().numpy())\n",
    "                                fig.add_subplot(1, 2, 2)\n",
    "                                plt.imshow(x_fake.cpu().detach().numpy()[j][0])\n",
    "                                print(property_label[j])\n",
    "                                print(torch.argmax(out_real_cls[j]))\n",
    "                                print(torch.argmax(out_fake_cls[j]))\n",
    "                                plt.show()\n",
    "            print(\"Epochs %s finished.\"%str(epoch).zfill(4))\n",
    "                    \n",
    "\n",
    "    def predictor(self):\n",
    "        \"\"\"\n",
    "\n",
    "        @return:\n",
    "        \"\"\"\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "brief-estate",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = HandWrittenGan(\n",
    "                 target_size=(64, 64),\n",
    "                 input_path='/home/vietvh9/Project/OCR_Components/data/sources/synthtext_font_lmdb',\n",
    "                 target_path='/home/vietvh9/Project/OCR_Components/data/sources/ETL_NIST_processed_v2.0_lmdb',\n",
    "                 classes=[str(i) for i in range(10)],\n",
    "                 batch_size=10,\n",
    "                 max_features=3,\n",
    "                 max_iter=1e+5,\n",
    "                 check_times = 1e+2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "taken-depression",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vietvh9/Project/.envs/GAN/lib/python3.6/site-packages/torch/nn/functional.py:1639: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/vietvh9/Project/.envs/GAN/lib/python3.6/site-packages/ipykernel_launcher.py:80: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/vietvh9/Project/.envs/GAN/lib/python3.6/site-packages/ipykernel_launcher.py:96: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0,       100], D_Loss:   2.477627, G_Loss:   9.236996, Mean Loss: 5.857311, Seconds/batch: 0.738024\n",
      "[  0,       200], D_Loss:   1.845061, G_Loss:   8.862033, Mean Loss: 5.353547, Seconds/batch: 0.579609\n",
      "[  0,       300], D_Loss:   1.250323, G_Loss:   9.032495, Mean Loss: 5.141409, Seconds/batch: 0.437869\n",
      "[  0,       400], D_Loss:   1.120466, G_Loss:   8.434822, Mean Loss: 4.777644, Seconds/batch: 0.387495\n",
      "[  0,       500], D_Loss:   0.955885, G_Loss:   8.175356, Mean Loss: 4.565621, Seconds/batch: 0.346299\n",
      "[  0,       600], D_Loss:   0.857539, G_Loss:   7.593907, Mean Loss: 4.225723, Seconds/batch: 0.355275\n",
      "[  0,       700], D_Loss:   0.801789, G_Loss:   7.605550, Mean Loss: 4.203669, Seconds/batch: 0.325750\n",
      "[  0,       800], D_Loss:   0.742560, G_Loss:   7.627130, Mean Loss: 4.184845, Seconds/batch: 0.339545\n",
      "[  0,       900], D_Loss:   0.647045, G_Loss:   7.366430, Mean Loss: 4.006738, Seconds/batch: 0.342566\n",
      "[  0,      1000], D_Loss:   0.660371, G_Loss:   7.463968, Mean Loss: 4.062170, Seconds/batch: 0.866757\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0,      1100], D_Loss:   0.574455, G_Loss:   7.736745, Mean Loss: 4.155600, Seconds/batch: 1.105901\n",
      "[  0,      1200], D_Loss:   0.507883, G_Loss:   7.602061, Mean Loss: 4.054972, Seconds/batch: 1.140985\n",
      "[  0,      1300], D_Loss:   0.497597, G_Loss:   7.177605, Mean Loss: 3.837601, Seconds/batch: 1.147421\n",
      "[  0,      1400], D_Loss:   0.500122, G_Loss:   7.157153, Mean Loss: 3.828637, Seconds/batch: 1.066403\n",
      "[  0,      1500], D_Loss:   0.455680, G_Loss:   7.317591, Mean Loss: 3.886635, Seconds/batch: 1.053941\n"
     ]
    }
   ],
   "source": [
    "models.trainer(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "celtic-essence",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[0.1414, 0.0866, 0.1143, 0.1201, 0.0862, 0.0844, 0.1058, 0.0871, 0.0762,\n",
    "         0.0977],\n",
    "        [0.2456, 0.0192, 0.1624, 0.1959, 0.0606, 0.0516, 0.0824, 0.0569, 0.0192,\n",
    "         0.1062],\n",
    "        [0.2344, 0.0240, 0.1467, 0.2067, 0.0627, 0.0563, 0.0861, 0.0587, 0.0227,\n",
    "         0.1017],\n",
    "        [0.2509, 0.0175, 0.1571, 0.2044, 0.0597, 0.0509, 0.0811, 0.0551, 0.0183,\n",
    "         0.1050],\n",
    "        [0.2332, 0.0192, 0.1614, 0.1983, 0.0632, 0.0540, 0.0800, 0.0587, 0.0195,\n",
    "         0.1125],\n",
    "        [0.2494, 0.0181, 0.1583, 0.1979, 0.0597, 0.0525, 0.0841, 0.0557, 0.0192,\n",
    "         0.1051],\n",
    "        [0.2445, 0.0182, 0.1573, 0.2014, 0.0632, 0.0518, 0.0793, 0.0561, 0.0192,\n",
    "         0.1090],\n",
    "        [0.2289, 0.0204, 0.1607, 0.1863, 0.0671, 0.0564, 0.0856, 0.0610, 0.0218,\n",
    "         0.1117],\n",
    "        [0.2207, 0.0241, 0.1619, 0.1893, 0.0660, 0.0568, 0.0853, 0.0632, 0.0224,\n",
    "         0.1102],\n",
    "        [0.2059, 0.0257, 0.1471, 0.1879, 0.0798, 0.0612, 0.0838, 0.0679, 0.0276,\n",
    "         0.1131]])\n",
    "b = torch.tensor([4, 3, 5, 3, 9, 9, 1, 1, 5, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mounted-audit",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "insured-warrior",
   "metadata": {},
   "outputs": [],
   "source": [
    "l(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developed-pottery",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
